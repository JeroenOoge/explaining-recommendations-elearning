{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = pd.read_csv('../Datasets/data/pre_study_questionnaire_updated.csv', sep=\",\")\n",
    "df = pd.read_csv('../Datasets/data/post_study_questionnaire_updated.csv', sep=\",\")\n",
    "rg_df = pd.read_csv('../Datasets/data/field_data_field_research_group_updated.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_df = rg_df.drop(['entity_type', 'bundle', 'deleted', 'revision_id', 'language', 'delta'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['UID']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Serienummer']>7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.merge(df,rg_df,how='inner', left_on='UID',right_on='entity_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>In welk leerjaar zit je?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1846</td>\n",
       "      <td>Anders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1847</td>\n",
       "      <td>Anders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1854</td>\n",
       "      <td>Anders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1864</td>\n",
       "      <td>Anders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1865</td>\n",
       "      <td>Anders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    UID In welk leerjaar zit je?\n",
       "0  1846                   Anders\n",
       "1  1847                   Anders\n",
       "2  1854                   Anders\n",
       "3  1864                   Anders\n",
       "4  1865                   Anders"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade_df = df_pre[['UID', 'In welk leerjaar zit je?']]\n",
    "grade_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.merge(result_df,grade_df,how='inner',left_on='UID',right_on='UID')\n",
    "result_df = result_df.drop_duplicates(subset=['UID']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df[result_df['In welk leerjaar zit je?']!=\" Anders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "12\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(result_df))\n",
    "print(len(result_df[result_df[\"field_research_group_value\"]==0]))\n",
    "print(len(result_df[result_df[\"field_research_group_value\"]==1]))\n",
    "print(len(result_df[result_df[\"field_research_group_value\"]==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = result_df[result_df[\"field_research_group_value\"]==0]\n",
    "placebo_df = result_df[result_df[\"field_research_group_value\"]==1]\n",
    "nothing_df = result_df[result_df[\"field_research_group_value\"]==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = '1. Wiski is zoals een expert (bv. een leerkracht) in wiskunde-oefeningen aanraden.'\n",
    "q2 = '2. Wiski heeft de expertise (kennis) om mijn wiskundeniveau te kunnen inschatten.'\n",
    "q3 = '3. Wiski kan mijn wiskundeniveau inschatten.'\n",
    "q4 = '4. Wiski begrijpt de moeilijkheidsgraad van wiskunde-oefeningen goed.'\n",
    "q5 = '5. Wiski houdt rekening met mijn wiskundeniveau om oefeningen aan te raden.'\n",
    "q6 = '6. Wiski zet op de eerste plaats dat ik vorderingen maak in wiskunde.'\n",
    "q7 = '7. Wanneer Wiski oefeningen aanraadt, doet Wiski dat zodat ik vorderingen maak in wiskunde.'\n",
    "q8 = '8. Wiski wilt mijn wiskundeniveau goed inschatten.'\n",
    "q9 = '9. Wiski raadt oefeningen op een zo correct mogelijke manier aan.'\n",
    "q10 = '10. Wiski is eerlijk.'\n",
    "q11 = '11. Wiski maakt oprechte aanbevelingen.'\n",
    "q12 = '12. Ik vertrouw Wiski om mij wiskunde-oefeningen aan te raden.'\n",
    "q13 = '13. Als ik nog eens online wiskunde-oefeningen maak, dan kies ik voor Wiski.'\n",
    "q14 = '14. Als ik nog eens wiskunde-oefeningen aangeraden wil krijgen, dan kies ik voor Wiski.'\n",
    "q15 = '15. Ik vind dat Wiski genoeg uitleg geeft over waarom een oefening aangeraden is.'\n",
    "q16 = '16. Wanneer ik Wiski gebruik, wil ik GEEN uitleg over waarom een oefening wordt aangeraden.'\n",
    "q17 = '17. Ik vind uitleg krijgen over waarom een oefening wordt aangeraden belangrijker dan waarom een film wordt aangeraden.'\n",
    "q18 = '18. Ik ben NIET blij met het niveau van de oefeningen die Wiski aanraadde.'\n",
    "q19 = '19. In het algemeen vind ik het belangrijk om uitleg te krijgen wanneer iets (oefening/film/product/...) wordt aangeraden.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' Eens': 9, ' Eerder eens': 2, ' Eerder oneens': 1}\n"
     ]
    }
   ],
   "source": [
    "# real explanations \n",
    "group_df = real_df\n",
    "rq1 = group_df[q1].value_counts().to_dict()\n",
    "rq2 = group_df[q2].value_counts().to_dict()\n",
    "rq3 = group_df[q3].value_counts().to_dict()\n",
    "rq4 = group_df[q4].value_counts().to_dict()\n",
    "rq5 = group_df[q5].value_counts().to_dict()\n",
    "rq6 = group_df[q6].value_counts().to_dict()\n",
    "rq7 = group_df[q7].value_counts().to_dict()\n",
    "rq8 = group_df[q8].value_counts().to_dict()\n",
    "rq9 = group_df[q9].value_counts().to_dict()\n",
    "rq10 = group_df[q10].value_counts().to_dict()\n",
    "rq11 = group_df[q11].value_counts().to_dict()\n",
    "rq12 = group_df[q12].value_counts().to_dict()\n",
    "rq13 = group_df[q13].value_counts().to_dict()\n",
    "rq14 = group_df[q14].value_counts().to_dict()\n",
    "rq15 = group_df[q15].value_counts().to_dict()\n",
    "rq16 = group_df[q16].value_counts().to_dict()\n",
    "rq17 = group_df[q17].value_counts().to_dict()\n",
    "rq18 = group_df[q18].value_counts().to_dict()\n",
    "rq19 = group_df[q19].value_counts().to_dict()\n",
    "print(rq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' Eens': 4, ' Eerder eens': 4, ' Oneens': 1, ' Helemaal eens': 1, ' Neutraal': 1, ' Helemaal oneens': 1}\n"
     ]
    }
   ],
   "source": [
    "# placebo explanations \n",
    "group_df = placebo_df\n",
    "pq1 = group_df[q1].value_counts().to_dict()\n",
    "pq2 = group_df[q2].value_counts().to_dict()\n",
    "pq3 = group_df[q3].value_counts().to_dict()\n",
    "pq4 = group_df[q4].value_counts().to_dict()\n",
    "pq5 = group_df[q5].value_counts().to_dict()\n",
    "pq6 = group_df[q6].value_counts().to_dict()\n",
    "pq7 = group_df[q7].value_counts().to_dict()\n",
    "pq8 = group_df[q8].value_counts().to_dict()\n",
    "pq9 = group_df[q9].value_counts().to_dict()\n",
    "pq10 = group_df[q10].value_counts().to_dict()\n",
    "pq11 = group_df[q11].value_counts().to_dict()\n",
    "pq12 = group_df[q12].value_counts().to_dict()\n",
    "pq13 = group_df[q13].value_counts().to_dict()\n",
    "pq14 = group_df[q14].value_counts().to_dict()\n",
    "pq15 = group_df[q15].value_counts().to_dict()\n",
    "pq16 = group_df[q16].value_counts().to_dict()\n",
    "pq17 = group_df[q17].value_counts().to_dict()\n",
    "pq18 = group_df[q18].value_counts().to_dict()\n",
    "pq19 = group_df[q19].value_counts().to_dict()\n",
    "print(pq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' Eerder eens': 5, ' Eens': 5, ' Neutraal': 3}\n"
     ]
    }
   ],
   "source": [
    "# no explanations \n",
    "group_df = nothing_df\n",
    "nq1 = group_df[q1].value_counts().to_dict()\n",
    "nq2 = group_df[q2].value_counts().to_dict()\n",
    "nq3 = group_df[q3].value_counts().to_dict()\n",
    "nq4 = group_df[q4].value_counts().to_dict()\n",
    "nq5 = group_df[q5].value_counts().to_dict()\n",
    "nq6 = group_df[q6].value_counts().to_dict()\n",
    "nq7 = group_df[q7].value_counts().to_dict()\n",
    "nq8 = group_df[q8].value_counts().to_dict()\n",
    "nq9 = group_df[q9].value_counts().to_dict()\n",
    "nq10 = group_df[q10].value_counts().to_dict()\n",
    "nq11 = group_df[q11].value_counts().to_dict()\n",
    "nq12 = group_df[q12].value_counts().to_dict()\n",
    "nq13 = group_df[q13].value_counts().to_dict()\n",
    "nq14 = group_df[q14].value_counts().to_dict()\n",
    "nq15 = group_df[q15].value_counts().to_dict()\n",
    "nq16 = group_df[q16].value_counts().to_dict()\n",
    "nq17 = group_df[q17].value_counts().to_dict()\n",
    "nq18 = group_df[q18].value_counts().to_dict()\n",
    "nq19 = group_df[q19].value_counts().to_dict()\n",
    "print(nq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_likert_to_int(d):\n",
    "    converted = dict()\n",
    "    for pair in d.items():\n",
    "        key = pair[0].strip()\n",
    "        value = pair[1]\n",
    "        if key == \"Helemaal oneens\":\n",
    "            converted[1] = int(value)\n",
    "        elif key == \"Oneens\":\n",
    "            converted[2] = int(value)\n",
    "        elif key == \"Eerder oneens\":\n",
    "            converted[3] = int(value)\n",
    "        elif key == \"Neutraal\":\n",
    "            converted[4] = int(value)\n",
    "        elif key == \"Eerder eens\":\n",
    "            converted[5] = int(value)\n",
    "        elif key == \"Eens\":\n",
    "            converted[6] = int(value)\n",
    "        elif key == \"Helemaal eens\":\n",
    "            converted[7] = int(value)\n",
    "        else:\n",
    "            print(key)\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: 9, 5: 2, 3: 1}\n"
     ]
    }
   ],
   "source": [
    "# real converted to values\n",
    "rqv1 = convert_likert_to_int(rq1)\n",
    "rqv2 = convert_likert_to_int(rq2)\n",
    "rqv3 = convert_likert_to_int(rq3)\n",
    "rqv4 = convert_likert_to_int(rq4)\n",
    "rqv5 = convert_likert_to_int(rq5)\n",
    "rqv6 = convert_likert_to_int(rq6)\n",
    "rqv7 = convert_likert_to_int(rq7)\n",
    "rqv8 = convert_likert_to_int(rq8)\n",
    "rqv9 = convert_likert_to_int(rq9)\n",
    "rqv10 = convert_likert_to_int(rq10)\n",
    "rqv11 = convert_likert_to_int(rq11)\n",
    "rqv12 = convert_likert_to_int(rq12)\n",
    "rqv13 = convert_likert_to_int(rq13)\n",
    "rqv14 = convert_likert_to_int(rq14)\n",
    "rqv15 = convert_likert_to_int(rq15)\n",
    "rqv16 = convert_likert_to_int(rq16)\n",
    "rqv17 = convert_likert_to_int(rq17)\n",
    "rqv18 = convert_likert_to_int(rq18)\n",
    "rqv19 = convert_likert_to_int(rq19)\n",
    "print(rqv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placebo converted to values\n",
    "pqv1 = convert_likert_to_int(pq1)\n",
    "pqv2 = convert_likert_to_int(pq2)\n",
    "pqv3 = convert_likert_to_int(pq3)\n",
    "pqv4 = convert_likert_to_int(pq4)\n",
    "pqv5 = convert_likert_to_int(pq5)\n",
    "pqv6 = convert_likert_to_int(pq6)\n",
    "pqv7 = convert_likert_to_int(pq7)\n",
    "pqv8 = convert_likert_to_int(pq8)\n",
    "pqv9 = convert_likert_to_int(pq9)\n",
    "pqv10 = convert_likert_to_int(pq10)\n",
    "pqv11 = convert_likert_to_int(pq11)\n",
    "pqv12 = convert_likert_to_int(pq12)\n",
    "pqv13 = convert_likert_to_int(pq13)\n",
    "pqv14 = convert_likert_to_int(pq14)\n",
    "pqv15 = convert_likert_to_int(pq15)\n",
    "pqv16 = convert_likert_to_int(pq16)\n",
    "pqv17 = convert_likert_to_int(pq17)\n",
    "pqv18 = convert_likert_to_int(pq18)\n",
    "pqv19 = convert_likert_to_int(pq19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing converted to values\n",
    "nqv1 = convert_likert_to_int(nq1)\n",
    "nqv2 = convert_likert_to_int(nq2)\n",
    "nqv3 = convert_likert_to_int(nq3)\n",
    "nqv4 = convert_likert_to_int(nq4)\n",
    "nqv5 = convert_likert_to_int(nq5)\n",
    "nqv6 = convert_likert_to_int(nq6)\n",
    "nqv7 = convert_likert_to_int(nq7)\n",
    "nqv8 = convert_likert_to_int(nq8)\n",
    "nqv9 = convert_likert_to_int(nq9)\n",
    "nqv10 = convert_likert_to_int(nq10)\n",
    "nqv11 = convert_likert_to_int(nq11)\n",
    "nqv12 = convert_likert_to_int(nq12)\n",
    "nqv13 = convert_likert_to_int(nq13)\n",
    "nqv14 = convert_likert_to_int(nq14)\n",
    "nqv15 = convert_likert_to_int(nq15)\n",
    "nqv16 = convert_likert_to_int(nq16)\n",
    "nqv17 = convert_likert_to_int(nq17)\n",
    "nqv18 = convert_likert_to_int(nq18)\n",
    "nqv19 = convert_likert_to_int(nq19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{6: 9, 5: 2, 3: 1}, {6: 6, 5: 3, 4: 1, 7: 1, 3: 1}, {6: 7, 7: 2, 5: 1, 4: 1, 2: 1}, {6: 7, 7: 2, 4: 2, 5: 1}, {7: 5, 6: 3, 5: 3, 3: 1}, {6: 8, 7: 2, 5: 2}, {6: 6, 5: 3, 4: 2, 7: 1}, {7: 6, 6: 6}, {6: 8, 5: 2, 7: 2}, {7: 6, 5: 4, 4: 1, 6: 1}, {7: 6, 6: 5, 4: 1}, {6: 5, 7: 4, 5: 2, 3: 1}, {6: 6, 5: 2, 7: 2, 4: 2}, {6: 8, 5: 2, 7: 1, 4: 1}, {7: 5, 4: 2, 3: 2, 5: 2, 6: 1}, {2: 4, 1: 2, 3: 2, 4: 2, 6: 1, 5: 1}, {4: 4, 5: 2, 7: 2, 2: 2, 3: 1, 1: 1}, {2: 5, 3: 3, 4: 2, 1: 1, 7: 1}, {6: 5, 4: 4, 5: 3}]\n"
     ]
    }
   ],
   "source": [
    "rqv = [rqv1,\n",
    "rqv2,\n",
    "rqv3,\n",
    "rqv4,\n",
    "rqv5,\n",
    "rqv6,\n",
    "rqv7,\n",
    "rqv8,\n",
    "rqv9,\n",
    "rqv10,\n",
    "rqv11,\n",
    "rqv12,\n",
    "rqv13,\n",
    "rqv14,\n",
    "rqv15,\n",
    "rqv16,\n",
    "rqv17,\n",
    "rqv18,\n",
    "rqv19]\n",
    "print(rqv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{6: 4, 5: 4, 2: 1, 7: 1, 4: 1, 1: 1}, {6: 4, 4: 3, 3: 2, 5: 2, 7: 1}, {6: 5, 4: 4, 5: 2, 3: 1}, {6: 5, 3: 3, 5: 2, 7: 1, 4: 1}, {6: 8, 5: 1, 3: 1, 4: 1, 7: 1}, {6: 4, 7: 3, 4: 3, 5: 2}, {6: 5, 5: 3, 7: 2, 2: 1, 4: 1}, {6: 8, 4: 2, 7: 1, 2: 1}, {6: 8, 4: 2, 7: 1, 3: 1}, {6: 6, 7: 2, 4: 2, 3: 1, 5: 1}, {6: 6, 4: 4, 7: 1, 3: 1}, {6: 6, 4: 2, 5: 1, 3: 1, 7: 1, 2: 1}, {6: 6, 5: 4, 4: 1, 2: 1}, {6: 7, 3: 2, 5: 1, 7: 1, 4: 1}, {5: 4, 3: 3, 6: 2, 4: 2, 2: 1}, {2: 5, 1: 3, 5: 2, 4: 2}, {6: 4, 5: 3, 4: 2, 3: 2, 2: 1}, {2: 5, 1: 4, 4: 3}, {5: 3, 6: 3, 4: 3, 3: 2, 7: 1}]\n"
     ]
    }
   ],
   "source": [
    "pqv = [pqv1,\n",
    "pqv2,\n",
    "pqv3,\n",
    "pqv4,\n",
    "pqv5,\n",
    "pqv6,\n",
    "pqv7,\n",
    "pqv8,\n",
    "pqv9,\n",
    "pqv10,\n",
    "pqv11,\n",
    "pqv12,\n",
    "pqv13,\n",
    "pqv14,\n",
    "pqv15,\n",
    "pqv16,\n",
    "pqv17,\n",
    "pqv18,\n",
    "pqv19]\n",
    "print(pqv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nqv = [nqv1,\n",
    "nqv2,\n",
    "nqv3,\n",
    "nqv4,\n",
    "nqv5,\n",
    "nqv6,\n",
    "nqv7,\n",
    "nqv8,\n",
    "nqv9,\n",
    "nqv10,\n",
    "nqv11,\n",
    "nqv12,\n",
    "nqv13,\n",
    "nqv14,\n",
    "nqv15,\n",
    "nqv16,\n",
    "nqv17,\n",
    "nqv18,\n",
    "nqv19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calclulate_values_per_question(list_of_dicts):\n",
    "    result = []\n",
    "    for dic in list_of_dicts:\n",
    "        to_add = 0\n",
    "        for pair in dic.items():\n",
    "            key = pair[0]\n",
    "            value = pair[1]\n",
    "            to_add += key * value\n",
    "        result.append(to_add)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_per_construct(lijst, num_students):\n",
    "    result = dict()\n",
    "    result[\"competence\"] = sum(lijst[0:5])/num_students\n",
    "    result[\"benevolence\"] = sum(lijst[5:8])/num_students\n",
    "    result[\"integrity\"] = sum(lijst[8:11])/num_students\n",
    "    result[\"explicit_trust\"] = lijst[11]/num_students\n",
    "    result[\"intention_to_return\"] = sum(lijst[12:14])/num_students\n",
    "    result[\"perceived_transparency\"] = lijst[14]/num_students\n",
    "    result[\"16\"] = lijst[15]/num_students\n",
    "    result[\"17\"] = lijst[16]/num_students  \n",
    "    result[\"18\"] = lijst[17]/num_students\n",
    "    result[\"19\"] = lijst[18]/num_students\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 65, 67, 69, 71, 72, 66, 78, 72, 72, 76, 71, 68, 69, 65, 35, 48, 35, 61]\n",
      "{'competence': 28.25, 'benevolence': 18.0, 'integrity': 18.333333333333332, 'explicit_trust': 5.916666666666667, 'intention_to_return': 11.416666666666666, 'perceived_transparency': 5.416666666666667, '16': 2.9166666666666665, '17': 4.0, '18': 2.9166666666666665, '19': 5.083333333333333}\n"
     ]
    }
   ],
   "source": [
    "real_values_per_question = calclulate_values_per_question(rqv)\n",
    "print(real_values_per_question)\n",
    "real_average_per_construct = calculate_average_per_construct(real_values_per_question, len(real_df))\n",
    "print(real_average_per_construct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 59, 59, 60, 67, 67, 65, 65, 66, 66, 62, 61, 62, 64, 51, 31, 55, 26, 58]\n",
      "{'competence': 25.25, 'benevolence': 16.416666666666668, 'integrity': 16.166666666666668, 'explicit_trust': 5.083333333333333, 'intention_to_return': 10.5, 'perceived_transparency': 4.25, '16': 2.5833333333333335, '17': 4.583333333333333, '18': 2.1666666666666665, '19': 4.833333333333333}\n"
     ]
    }
   ],
   "source": [
    "placebo_values_per_question = calclulate_values_per_question(pqv)\n",
    "print(placebo_values_per_question)\n",
    "placebo_average_per_construct = calculate_average_per_construct(placebo_values_per_question, len(placebo_df))\n",
    "print(placebo_average_per_construct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 66, 66, 64, 72, 71, 67, 75, 73, 84, 75, 69, 68, 69, 44, 32, 54, 32, 72]\n",
      "{'competence': 25.76923076923077, 'benevolence': 16.384615384615383, 'integrity': 17.846153846153847, 'explicit_trust': 5.3076923076923075, 'intention_to_return': 10.538461538461538, 'perceived_transparency': 3.3846153846153846, '16': 2.4615384615384617, '17': 4.153846153846154, '18': 2.4615384615384617, '19': 5.538461538461538}\n"
     ]
    }
   ],
   "source": [
    "nothing_values_per_question = calclulate_values_per_question(nqv)\n",
    "print(nothing_values_per_question)\n",
    "nothing_average_per_construct = calculate_average_per_construct(nothing_values_per_question, len(nothing_df))\n",
    "print(nothing_average_per_construct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ik heb niet echt uitleg gezien. ',\n",
       "       'er wordt niet echt vermeldt waarom de oefening aangeraden wordt. (bv: omdat het hetzelfde/ moeilijker/makkelijk niveau heeft)',\n",
       "       'Misschien heb ik erover gekeken maar ik heb nog niet echt een uitleg hierover gezien.',\n",
       "       'Ik heb nog geen uitleg gezien maar misschien ben ik het gewoon overgeslagen',\n",
       "       'ik vind het heel handig dat er gemiddeld en moeilijk enz bij de oefeningen staan.',\n",
       "       'Ik heb dit precies gemist',\n",
       "       'Ik heb nog geen uitleg gezien, maar ik heb er ook nog geen nodig gehad.',\n",
       "       'Ik heb nergens gelezen waarom een oefening aan mij werd aangeraden. ',\n",
       "       'Als je een nieuwe oefening wilt maken is het handig dat je weet waarom deze oefening aangeraden wordt, dit doet de website goed.',\n",
       "       'Ik kreeg geen uitleg waarom het werd aangeraden',\n",
       "       'Ik heb geen uitleg gezien. Er stond gewoon dat het aangereden werd. ',\n",
       "       'Ja ik vind dat er genoeg uitleg is.',\n",
       "       \"Ik kan nergens echt vinden 'waarom' een oefening wordt aangeraden.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nothing_df['Motiveer (verplicht) jouw antwoord op vraag 15.'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests\n",
    "- Non parametric tests: Mann Whitney, Kruskal Wallis, ... -> No need for normal distribution but need enough people\n",
    "- Parametric test: t-test -> Small number of people ok but need normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# rqv\n",
    "# pqv\n",
    "# nqv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_likert_to_int(key):\n",
    "    if key == \"Helemaal oneens\":\n",
    "        return 1\n",
    "    elif key == \"Oneens\":\n",
    "        return 2\n",
    "    elif key == \"Eerder oneens\":\n",
    "        return 3\n",
    "    elif key == \"Neutraal\":\n",
    "        return 4\n",
    "    elif key == \"Eerder eens\":\n",
    "        return 5\n",
    "    elif key == \"Eens\":\n",
    "        return 6\n",
    "    elif key == \"Helemaal eens\":\n",
    "        return 7\n",
    "def list_scores_competence(df):\n",
    "    competence_scores = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        score = 0\n",
    "        score += int(map_likert_to_int(row[q1].strip()))\n",
    "        score += int(map_likert_to_int(row[q2].strip()))\n",
    "        score += int(map_likert_to_int(row[q3].strip()))\n",
    "        score += int(map_likert_to_int(row[q4].strip()))\n",
    "        score += int(map_likert_to_int(row[q5].strip()))\n",
    "        competence_scores.append(score)\n",
    "    return competence_scores\n",
    "def list_scores_benevolence(df):\n",
    "    benevolence_scores = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        score = 0\n",
    "        score += int(map_likert_to_int(row[q6].strip()))\n",
    "        score += int(map_likert_to_int(row[q7].strip()))\n",
    "        score += int(map_likert_to_int(row[q8].strip()))\n",
    "        benevolence_scores.append(score)\n",
    "    return benevolence_scores\n",
    "def list_scores_integrity(df):\n",
    "    integrity_scores = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        score = 0\n",
    "        score += int(map_likert_to_int(row[q9].strip()))\n",
    "        score += int(map_likert_to_int(row[q10].strip()))\n",
    "        score += int(map_likert_to_int(row[q11].strip()))\n",
    "        integrity_scores.append(score)\n",
    "    return integrity_scores       \n",
    "def list_scores_trusting_beliefs(competence, benevolence, integrity):\n",
    "    trusting_beliefs = [competence[i]/5+benevolence[i]/3+integrity[i]/3 for i in range(len(competence))]\n",
    "    return trusting_beliefs\n",
    "def list_scores_perceived_transparency(df):\n",
    "    pt_scores = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        score = 0\n",
    "        score += int(map_likert_to_int(row[q15].strip()))\n",
    "        pt_scores.append(score)\n",
    "    return pt_scores\n",
    "def list_scores_intention_to_return(df):\n",
    "    itr_scores = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        score = 0\n",
    "        score += int(map_likert_to_int(row[q13].strip()))\n",
    "        score += int(map_likert_to_int(row[q14].strip()))\n",
    "        itr_scores.append(score)\n",
    "    return itr_scores\n",
    "def list_scores_single_q(df, q_number):\n",
    "    scores = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        score = 0\n",
    "        score += int(map_likert_to_int(row[q_number].strip()))\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "def list_scores_md_trust(trusting_beliefs, intention_to_return, perceived_transparency):\n",
    "    return [trusting_beliefs[i]/3 + intention_to_return[i]/2 + perceived_transparency[i] for i in range(len(trusting_beliefs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.3999999999999995, 6.177777777777778, 5.866666666666667, 6.8, 5.133333333333333, 5.888888888888889, 6.288888888888889, 6.066666666666666, 6.177777777777778, 5.8, 6.222222222222222, 4.222222222222222]\n"
     ]
    }
   ],
   "source": [
    "competence_scores_real = list_scores_competence(real_df)\n",
    "competence_scores_placebo = list_scores_competence(placebo_df)\n",
    "competence_scores_nothing = list_scores_competence(nothing_df)\n",
    "competence_scores = [competence_scores_real, competence_scores_placebo, competence_scores_nothing]\n",
    "\n",
    "\n",
    "benevolence_scores_real = list_scores_benevolence(real_df)\n",
    "benevolence_scores_placebo = list_scores_benevolence(placebo_df)\n",
    "benevolence_scores_nothing = list_scores_benevolence(nothing_df)\n",
    "benevolence_scores = [benevolence_scores_real, benevolence_scores_placebo, benevolence_scores_nothing]\n",
    "\n",
    "\n",
    "integrity_scores_real = list_scores_integrity(real_df)\n",
    "integrity_scores_placebo = list_scores_integrity(placebo_df)\n",
    "integrity_scores_nothing = list_scores_integrity(nothing_df)\n",
    "integrity_scores = [integrity_scores_real, integrity_scores_placebo, integrity_scores_nothing]\n",
    "\n",
    "\n",
    "trusting_beliefs_scores_real = list_scores_trusting_beliefs(competence_scores_real, benevolence_scores_real, integrity_scores_real)\n",
    "trusting_beliefs_scores_placebo = list_scores_trusting_beliefs(competence_scores_placebo, benevolence_scores_placebo, integrity_scores_placebo)\n",
    "trusting_beliefs_scores_nothing = list_scores_trusting_beliefs(competence_scores_nothing, benevolence_scores_nothing, integrity_scores_nothing)\n",
    "trusting_beliefs_scores = [trusting_beliefs_scores_real, trusting_beliefs_scores_placebo, trusting_beliefs_scores_nothing]\n",
    "\n",
    "\n",
    "itr_scores_real = list_scores_intention_to_return(real_df)\n",
    "itr_scores_placebo = list_scores_intention_to_return(placebo_df)\n",
    "itr_scores_nothing = list_scores_intention_to_return(nothing_df)\n",
    "itr_scores = [itr_scores_real, itr_scores_placebo, itr_scores_nothing]\n",
    "\n",
    "\n",
    "pt_scores_real = list_scores_perceived_transparency(real_df)\n",
    "pt_scores_placebo = list_scores_perceived_transparency(placebo_df)\n",
    "pt_scores_nothing = list_scores_perceived_transparency(nothing_df)\n",
    "pt_scores = [pt_scores_real, pt_scores_placebo, pt_scores_nothing]\n",
    "\n",
    "explicit_trust_scores_real = list_scores_single_q(real_df, q12)\n",
    "explicit_trust_scores_placebo = list_scores_single_q(placebo_df, q12)\n",
    "explicit_trust_scores_nothing = list_scores_single_q(nothing_df, q12)\n",
    "explicit_trust_scores = [explicit_trust_scores_real, explicit_trust_scores_placebo, explicit_trust_scores_nothing]\n",
    "\n",
    "\n",
    "md_trust_scores_real = list_scores_md_trust(trusting_beliefs_scores_real, itr_scores_real, pt_scores_real)\n",
    "md_trust_scores_placebo = list_scores_md_trust(trusting_beliefs_scores_placebo, itr_scores_placebo, pt_scores_placebo)\n",
    "md_trust_scores_nothing = list_scores_md_trust(trusting_beliefs_scores_nothing, itr_scores_nothing, pt_scores_nothing)\n",
    "md_trust_scores = [md_trust_scores_real, md_trust_scores_placebo, md_trust_scores_nothing]\n",
    "print([i/3 for i in trusting_beliefs_scores_real])\n",
    "\n",
    "q16_scores_real = list_scores_single_q(real_df, q16)\n",
    "q16_scores_placebo = list_scores_single_q(placebo_df, q16)\n",
    "q16_scores_nothing = list_scores_single_q(nothing_df, q16)\n",
    "q16_scores = [q16_scores_real, q16_scores_placebo, q16_scores_nothing]\n",
    "\n",
    "\n",
    "q17_scores_real = list_scores_single_q(real_df, q17)\n",
    "q17_scores_placebo = list_scores_single_q(placebo_df, q17)\n",
    "q17_scores_nothing = list_scores_single_q(nothing_df, q17)\n",
    "q17_scores = [q17_scores_real, q17_scores_placebo, q17_scores_nothing]\n",
    "\n",
    "\n",
    "q18_scores_real = list_scores_single_q(real_df, q18)\n",
    "q18_scores_placebo = list_scores_single_q(placebo_df, q18)\n",
    "q18_scores_nothing = list_scores_single_q(nothing_df, q18)\n",
    "q18_scores = [q18_scores_real, q18_scores_placebo, q18_scores_nothing]\n",
    "\n",
    "\n",
    "q19_scores_real = list_scores_single_q(real_df, q19)\n",
    "q19_scores_placebo = list_scores_single_q(placebo_df, q19)\n",
    "q19_scores_nothing = list_scores_single_q(nothing_df, q19)\n",
    "q19_scores = [q19_scores_real, q19_scores_placebo, q19_scores_nothing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs, cp = stats.ttest_ind(competence_scores_real,competence_scores_placebo)\n",
    "bs, bp = stats.ttest_ind(benevolence_scores_real,benevolence_scores_placebo)\n",
    "ins, inp = stats.ttest_ind(integrity_scores_real,integrity_scores_placebo)\n",
    "tbs, tbp = stats.ttest_ind(trusting_beliefs_scores_real,trusting_beliefs_scores_placebo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann-Whitney U Tests with Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real vs Placebo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_cs, mwu_cp = stats.mannwhitneyu(competence_scores_real,competence_scores_placebo)\n",
    "mwu_bs, mwu_bp = stats.mannwhitneyu(benevolence_scores_real,benevolence_scores_placebo)\n",
    "mwu_ins, mwu_inp = stats.mannwhitneyu(integrity_scores_real,integrity_scores_placebo)\n",
    "mwu_tbs, mwu_tbp = stats.mannwhitneyu(trusting_beliefs_scores_real,trusting_beliefs_scores_placebo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U Test P-Values (Real vs Placebo)\n",
      "Competence: 0.04683640427508711\n",
      "Benevolence: 0.14855394344532039\n",
      "Integrity: 0.10796616797087553\n",
      "Trusting Beliefs: 0.052942943315271045\n"
     ]
    }
   ],
   "source": [
    "print(\"Mann-Whitney U Test P-Values (Real vs Placebo)\")\n",
    "print(\"Competence: \" + str(mwu_cp))\n",
    "print(\"Benevolence: \" + str(mwu_bp))\n",
    "print(\"Integrity: \" + str(mwu_inp))\n",
    "print(\"Trusting Beliefs: \" + str(mwu_tbp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real vs No Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U Test P-Values (Real vs No Explanation)\n",
      "Competence: 0.05904732661291849\n",
      "Benevolence: 0.06011157458194808\n",
      "Integrity: 0.5227861349868247\n",
      "Trusting Beliefs: 0.09692667882453078\n"
     ]
    }
   ],
   "source": [
    "mwu_rn_cs, mwu_rn_cp = stats.mannwhitneyu(competence_scores_real,competence_scores_nothing)\n",
    "mwu_rn_bs, mwu_rn_bp = stats.mannwhitneyu(benevolence_scores_real,benevolence_scores_nothing)\n",
    "mwu_rn_ins, mwu_rn_inp = stats.mannwhitneyu(integrity_scores_real,integrity_scores_nothing)\n",
    "mwu_rn_tbs, mwu_rn_tbp = stats.mannwhitneyu(trusting_beliefs_scores_real,trusting_beliefs_scores_nothing)\n",
    "\n",
    "print(\"Mann-Whitney U Test P-Values (Real vs No Explanation)\")\n",
    "print(\"Competence: \" + str(mwu_rn_cp))\n",
    "print(\"Benevolence: \" + str(mwu_rn_bp))\n",
    "print(\"Integrity: \" + str(mwu_rn_inp))\n",
    "print(\"Trusting Beliefs: \" + str(mwu_rn_tbp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_language_effect_size(U, n1, n2):\n",
    "    return U/(n1*n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U Test U-Values (Real vs No Explanation)\n",
      "Competence: 113.0\n",
      "Benevolence: 112.5\n",
      "Integrity: 90.0\n",
      "Trusting Beliefs: 109.0\n",
      "Mann-Whitney U Test Common Language Effect Size (Real vs No Explanation)\n",
      "Competence: 0.7243589743589743\n",
      "Benevolence: 0.7211538461538461\n",
      "Integrity: 0.5769230769230769\n",
      "Trusting Beliefs: 0.6987179487179487\n"
     ]
    }
   ],
   "source": [
    "real_n = len(real_df)\n",
    "placebo_n = len(placebo_df)\n",
    "nothing_n = len(nothing_df)\n",
    "\n",
    "print(\"Mann-Whitney U Test U-Values (Real vs No Explanation)\")\n",
    "print(\"Competence: \" + str(mwu_rn_cs))\n",
    "print(\"Benevolence: \" + str(mwu_rn_bs))\n",
    "print(\"Integrity: \" + str(mwu_rn_ins))\n",
    "print(\"Trusting Beliefs: \" + str(mwu_rn_tbs))\n",
    "\n",
    "print(\"Mann-Whitney U Test Common Language Effect Size (Real vs No Explanation)\")\n",
    "print(\"Competence: \" + str(common_language_effect_size(mwu_rn_cs, real_n, nothing_n)))\n",
    "print(\"Benevolence: \" + str(common_language_effect_size(mwu_rn_bs, real_n, nothing_n)))\n",
    "print(\"Integrity: \" + str(common_language_effect_size(mwu_rn_ins, real_n, nothing_n)))\n",
    "print(\"Trusting Beliefs: \" + str(common_language_effect_size(mwu_rn_tbs, real_n, nothing_n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann-Whitney U Tests with Pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mwu(group1, group2, tail=\"one-sided\"):\n",
    "    vars = [\"competence\", \"benevolence\", \"integrity\", \"trusting_beliefs\", \"itr\", \"pt\", \"explicit_trust\", \"md_trust\"]\n",
    "    df = pd.DataFrame(columns=[\"p\", \"U\", \"CLES\", \"tail\"])\n",
    "    for var in vars:\n",
    "        data1 = var + \"_scores_\" + group1\n",
    "        data2 = var + \"_scores_\" + group2\n",
    "        mwu = pg.mwu(eval(data1),eval(data2),tail)\n",
    "        df.loc[var] = [float(mwu[\"p-val\"]), float(mwu[\"U-val\"]), float(mwu[\"CLES\"])] + [mwu[\"tail\"].MWU]\n",
    "    return round(df,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL vs NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>U</th>\n",
       "      <th>CLES</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>competence</th>\n",
       "      <td>0.030</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.724</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benevolence</th>\n",
       "      <td>0.030</td>\n",
       "      <td>112.5</td>\n",
       "      <td>0.721</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>integrity</th>\n",
       "      <td>0.261</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.577</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trusting_beliefs</th>\n",
       "      <td>0.048</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.699</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr</th>\n",
       "      <td>0.109</td>\n",
       "      <td>100.5</td>\n",
       "      <td>0.644</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>0.002</td>\n",
       "      <td>130.5</td>\n",
       "      <td>0.837</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explicit_trust</th>\n",
       "      <td>0.137</td>\n",
       "      <td>97.5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>md_trust</th>\n",
       "      <td>0.002</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.840</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      p      U   CLES     tail\n",
       "competence        0.030  113.0  0.724  greater\n",
       "benevolence       0.030  112.5  0.721  greater\n",
       "integrity         0.261   90.0  0.577  greater\n",
       "trusting_beliefs  0.048  109.0  0.699  greater\n",
       "itr               0.109  100.5  0.644  greater\n",
       "pt                0.002  130.5  0.837  greater\n",
       "explicit_trust    0.137   97.5  0.625  greater\n",
       "md_trust          0.002  131.0  0.840  greater"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwu(\"real\", \"nothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL vs PLACEBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>U</th>\n",
       "      <th>CLES</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>competence</th>\n",
       "      <td>0.023</td>\n",
       "      <td>106.5</td>\n",
       "      <td>0.740</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benevolence</th>\n",
       "      <td>0.074</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.674</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>integrity</th>\n",
       "      <td>0.054</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.694</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trusting_beliefs</th>\n",
       "      <td>0.026</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.736</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr</th>\n",
       "      <td>0.139</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>0.041</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.708</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explicit_trust</th>\n",
       "      <td>0.071</td>\n",
       "      <td>96.5</td>\n",
       "      <td>0.670</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>md_trust</th>\n",
       "      <td>0.013</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.771</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      p      U   CLES     tail\n",
       "competence        0.023  106.5  0.740  greater\n",
       "benevolence       0.074   97.0  0.674  greater\n",
       "integrity         0.054  100.0  0.694  greater\n",
       "trusting_beliefs  0.026  106.0  0.736  greater\n",
       "itr               0.139   90.0  0.625  greater\n",
       "pt                0.041  102.0  0.708  greater\n",
       "explicit_trust    0.071   96.5  0.670  greater\n",
       "md_trust          0.013  111.0  0.771  greater"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwu(\"real\", \"placebo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLACEBO vs NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>U</th>\n",
       "      <th>CLES</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>competence</th>\n",
       "      <td>1.000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>two-sided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benevolence</th>\n",
       "      <td>1.000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>two-sided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>integrity</th>\n",
       "      <td>0.143</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.327</td>\n",
       "      <td>two-sided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trusting_beliefs</th>\n",
       "      <td>0.663</td>\n",
       "      <td>69.5</td>\n",
       "      <td>0.446</td>\n",
       "      <td>two-sided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr</th>\n",
       "      <td>0.696</td>\n",
       "      <td>85.5</td>\n",
       "      <td>0.548</td>\n",
       "      <td>two-sided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>0.099</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.692</td>\n",
       "      <td>two-sided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explicit_trust</th>\n",
       "      <td>0.728</td>\n",
       "      <td>71.5</td>\n",
       "      <td>0.458</td>\n",
       "      <td>two-sided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>md_trust</th>\n",
       "      <td>0.327</td>\n",
       "      <td>96.5</td>\n",
       "      <td>0.619</td>\n",
       "      <td>two-sided</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      p      U   CLES       tail\n",
       "competence        1.000   78.0  0.500  two-sided\n",
       "benevolence       1.000   78.0  0.500  two-sided\n",
       "integrity         0.143   51.0  0.327  two-sided\n",
       "trusting_beliefs  0.663   69.5  0.446  two-sided\n",
       "itr               0.696   85.5  0.548  two-sided\n",
       "pt                0.099  108.0  0.692  two-sided\n",
       "explicit_trust    0.728   71.5  0.458  two-sided\n",
       "md_trust          0.327   96.5  0.619  two-sided"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwu(\"placebo\", \"nothing\", \"two-sided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests for Acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv(\"../Datasets/likert_response_and_acceptance_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "accep_df = master_df[[\"uid\",\"Acceptance\", \"Research Group\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance_good_df=accep_df[~accep_df[\"uid\"].isin({1921, 1842, 1892, 1958, 1944, 1950})]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>U-val</th>\n",
       "      <th>tail</th>\n",
       "      <th>p-val</th>\n",
       "      <th>RBC</th>\n",
       "      <th>CLES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MWU</th>\n",
       "      <td>67.0</td>\n",
       "      <td>greater</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>-0.654321</td>\n",
       "      <td>0.82716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     U-val     tail     p-val       RBC     CLES\n",
       "MWU   67.0  greater  0.007117 -0.654321  0.82716"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg.mwu(acceptance_good_df[acceptance_good_df[\"Research Group\"]==\"IRE\"][\"Acceptance\"], acceptance_good_df[acceptance_good_df[\"Research Group\"]==\"INE\"][\"Acceptance\"], tail=\"one-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>U-val</th>\n",
       "      <th>tail</th>\n",
       "      <th>p-val</th>\n",
       "      <th>RBC</th>\n",
       "      <th>CLES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MWU</th>\n",
       "      <td>72.0</td>\n",
       "      <td>greater</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     U-val     tail     p-val       RBC      CLES\n",
       "MWU   72.0  greater  0.038551 -0.454545  0.727273"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg.mwu(acceptance_good_df[acceptance_good_df[\"Research Group\"]==\"IPE\"][\"Acceptance\"], acceptance_good_df[acceptance_good_df[\"Research Group\"]==\"INE\"][\"Acceptance\"], tail=\"one-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>U-val</th>\n",
       "      <th>tail</th>\n",
       "      <th>p-val</th>\n",
       "      <th>RBC</th>\n",
       "      <th>CLES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MWU</th>\n",
       "      <td>60.0</td>\n",
       "      <td>greater</td>\n",
       "      <td>0.184972</td>\n",
       "      <td>-0.212121</td>\n",
       "      <td>0.606061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     U-val     tail     p-val       RBC      CLES\n",
       "MWU   60.0  greater  0.184972 -0.212121  0.606061"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg.mwu(acceptance_good_df[acceptance_good_df[\"Research Group\"]==\"IRE\"][\"Acceptance\"], acceptance_good_df[acceptance_good_df[\"Research Group\"]==\"IPE\"][\"Acceptance\"], tail=\"one-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(qv):\n",
    "    result = [] # each index is the question. each element is list of likert responses\n",
    "    for dic in qv: # get amount for each question\n",
    "        to_add = []\n",
    "        for i in range(1,8): # [#helemaal oneens, ... , #helemaal eens]\n",
    "            amount = dic.get(i, 0)\n",
    "            to_add.append(amount)\n",
    "        result.append(to_add)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 0, 2, 9, 0], [0, 0, 1, 1, 3, 6, 1], [0, 1, 0, 1, 1, 7, 2], [0, 0, 0, 2, 1, 7, 2], [0, 0, 1, 0, 3, 3, 5], [0, 0, 0, 0, 2, 8, 2], [0, 0, 0, 2, 3, 6, 1], [0, 0, 0, 0, 0, 6, 6], [0, 0, 0, 0, 2, 8, 2], [0, 0, 0, 1, 4, 1, 6], [0, 0, 0, 1, 0, 5, 6], [0, 0, 1, 0, 2, 5, 4], [0, 0, 0, 2, 2, 6, 2], [0, 0, 0, 1, 2, 8, 1], [0, 0, 2, 2, 2, 1, 5], [2, 4, 2, 2, 1, 1, 0], [1, 2, 1, 4, 2, 0, 2], [1, 5, 3, 2, 0, 0, 1], [0, 0, 0, 4, 3, 5, 0]]\n",
      "[[1, 1, 0, 1, 4, 4, 1], [0, 0, 2, 3, 2, 4, 1], [0, 0, 1, 4, 2, 5, 0], [0, 0, 3, 1, 2, 5, 1], [0, 0, 1, 1, 1, 8, 1], [0, 0, 0, 3, 2, 4, 3], [0, 1, 0, 1, 3, 5, 2], [0, 1, 0, 2, 0, 8, 1], [0, 0, 1, 2, 0, 8, 1], [0, 0, 1, 2, 1, 6, 2], [0, 0, 1, 4, 0, 6, 1], [0, 1, 1, 2, 1, 6, 1], [0, 1, 0, 1, 4, 6, 0], [0, 0, 2, 1, 1, 7, 1], [0, 1, 3, 2, 4, 2, 0], [3, 5, 0, 2, 2, 0, 0], [0, 1, 2, 2, 3, 4, 0], [4, 5, 0, 3, 0, 0, 0], [0, 0, 2, 3, 3, 3, 1]]\n",
      "[[0, 0, 0, 3, 5, 5, 0], [0, 0, 2, 1, 5, 4, 1], [0, 0, 2, 1, 5, 4, 1], [0, 0, 3, 0, 5, 5, 0], [0, 0, 1, 1, 4, 4, 3], [0, 0, 0, 2, 3, 8, 0], [0, 0, 1, 1, 6, 5, 0], [0, 0, 1, 0, 3, 6, 3], [0, 0, 0, 1, 4, 7, 1], [0, 0, 0, 0, 1, 5, 7], [0, 0, 1, 1, 1, 7, 3], [0, 0, 3, 0, 2, 6, 2], [0, 0, 0, 2, 7, 3, 1], [0, 0, 1, 2, 5, 2, 3], [0, 3, 5, 3, 1, 1, 0], [4, 3, 2, 4, 0, 0, 0], [0, 1, 2, 5, 4, 1, 0], [4, 5, 1, 1, 1, 1, 0], [0, 0, 0, 2, 4, 5, 2]]\n"
     ]
    }
   ],
   "source": [
    "likert_responses_per_question_real = extract_data(rqv)\n",
    "likert_responses_per_question_placebo = extract_data(pqv)\n",
    "likert_responses_per_question_nothing = extract_data(nqv)\n",
    "print(likert_responses_per_question_real)\n",
    "print(likert_responses_per_question_placebo)\n",
    "print(likert_responses_per_question_nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_q1_4 = [likert_responses_per_question_real[0][4], likert_responses_per_question_placebo[0][4], likert_responses_per_question_nothing[0][4]]\n",
    "test_q1_5 = [likert_responses_per_question_real[0][5], likert_responses_per_question_placebo[0][5], likert_responses_per_question_nothing[0][5]]\n",
    "plt.figure()\n",
    "plt.hist([test_q1_4,test_q1_5], 2, stacked=True, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>U-val</th>\n",
       "      <th>tail</th>\n",
       "      <th>p-val</th>\n",
       "      <th>RBC</th>\n",
       "      <th>CLES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MWU</th>\n",
       "      <td>97.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.295036</td>\n",
       "      <td>-0.24359</td>\n",
       "      <td>0.621795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     U-val       tail     p-val      RBC      CLES\n",
       "MWU   97.0  two-sided  0.295036 -0.24359  0.621795"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg.mwu(q18_scores_real,q18_scores_nothing, tail='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>U-val</th>\n",
       "      <th>tail</th>\n",
       "      <th>p-val</th>\n",
       "      <th>RBC</th>\n",
       "      <th>CLES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MWU</th>\n",
       "      <td>72.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.752552</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     U-val       tail     p-val       RBC      CLES\n",
       "MWU   72.0  two-sided  0.752552  0.076923  0.461538"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg.mwu(q18_scores_placebo,q18_scores_nothing, tail='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>U-val</th>\n",
       "      <th>tail</th>\n",
       "      <th>p-val</th>\n",
       "      <th>RBC</th>\n",
       "      <th>CLES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MWU</th>\n",
       "      <td>94.5</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.182815</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>0.65625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     U-val       tail     p-val     RBC     CLES\n",
       "MWU   94.5  two-sided  0.182815 -0.3125  0.65625"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg.mwu(q18_scores_real,q18_scores_placebo, tail='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competence\n",
      "[28.25, 25.25, 25.76923076923077]\n",
      "[30.5, 26.0, 27]\n",
      "[24.931818181818183, 24.022727272727273, 20.358974358974358]\n",
      "Benevolence\n",
      "[18, 16.416666666666668, 16.384615384615383]\n",
      "[18.0, 17.0, 16]\n",
      "[2.727272727272727, 7.174242424242424, 3.7564102564102564]\n",
      "Integrity\n",
      "[18.333333333333332, 16.166666666666668, 17.846153846153847]\n",
      "[19.0, 17.0, 18]\n",
      "[4.606060606060606, 10.515151515151516, 4.4743589743589745]\n",
      "Intention to return\n",
      "[11.416666666666666, 10.5, 10.538461538461538]\n",
      "[12.0, 11.5, 10]\n",
      "[2.628787878787879, 4.818181818181818, 3.6025641025641026]\n",
      "Perceived Transparency\n",
      "[5.416666666666667, 4.25, 3.3846153846153846]\n",
      "[5.5, 4.5, 3]\n",
      "[2.628787878787879, 1.6590909090909092, 1.4230769230769231]\n",
      "Trusting Beliefs\n",
      "[17.761111111111113, 15.911111111111111, 16.564102564102562]\n",
      "[18.366666666666667, 16.833333333333332, 16.400000000000002]\n",
      "[4.0141077441077435, 7.425723905723906, 3.4249002849002856]\n",
      "Explicit Trust\n",
      "[5.916666666666667, 5.083333333333333, 5.3076923076923075]\n",
      "[6.0, 6.0, 6]\n",
      "[1.3560606060606062, 2.265151515151515, 2.064102564102564]\n",
      "MD Trust\n",
      "[17.04537037037037, 14.803703703703704, 14.175213675213675]\n",
      "[17.511111111111113, 15.333333333333334, 14.355555555555554]\n",
      "[6.709774597830154, 4.724070332959222, 1.9579661285216834]\n",
      "Q16\n",
      "[2.9166666666666665, 2.5833333333333335, 2.4615384615384617]\n",
      "[2.5, 2.0, 2]\n",
      "Q17\n",
      "[4, 4.583333333333333, 4.153846153846154]\n",
      "[4.0, 5.0, 4]\n",
      "Q18\n",
      "[2.9166666666666665, 2.1666666666666665, 2.4615384615384617]\n",
      "[2.5, 2.0, 2]\n",
      "Q19\n",
      "[5.083333333333333, 4.833333333333333, 5.538461538461538]\n",
      "[5.0, 5.0, 6]\n",
      "Q16 All Round\n",
      "2.6486486486486487\n",
      "2\n",
      "Q17 All Round\n",
      "4.243243243243243\n",
      "4\n",
      "Q18 All Round\n",
      "2.5135135135135136\n",
      "2\n",
      "Q19 All Round\n",
      "5.162162162162162\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "competence_averages = [statistics.mean(competence_scores[i]) for i in range(len(competence_scores))]\n",
    "competence_median = [statistics.median(competence_scores[i]) for i in range(len(competence_scores))]\n",
    "competence_variance = [statistics.variance(competence_scores[i]) for i in range(len(competence_scores))]\n",
    "\n",
    "\n",
    "benevolence_averages = [statistics.mean(benevolence_scores[i]) for i in range(len(benevolence_scores))]\n",
    "benevolence_median = [statistics.median(benevolence_scores[i]) for i in range(len(benevolence_scores))]\n",
    "benevolence_variance = [statistics.variance(benevolence_scores[i]) for i in range(len(benevolence_scores))]\n",
    "\n",
    "\n",
    "integrity_averages = [statistics.mean(integrity_scores[i]) for i in range(len(integrity_scores))]\n",
    "integrity_median = [statistics.median(integrity_scores[i]) for i in range(len(integrity_scores))]\n",
    "integrity_variance = [statistics.variance(integrity_scores[i]) for i in range(len(integrity_scores))]\n",
    "\n",
    "\n",
    "itr_averages = [statistics.mean(itr_scores[i]) for i in range(len(itr_scores))]\n",
    "itr_median = [statistics.median(itr_scores[i]) for i in range(len(itr_scores))]\n",
    "itr_variance = [statistics.variance(itr_scores[i]) for i in range(len(itr_scores))]\n",
    "\n",
    "\n",
    "pt_averages = [statistics.mean(pt_scores[i]) for i in range(len(pt_scores))]\n",
    "pt_median = [statistics.median(pt_scores[i]) for i in range(len(pt_scores))]\n",
    "pt_variance = [statistics.variance(pt_scores[i]) for i in range(len(pt_scores))]\n",
    "\n",
    "\n",
    "\n",
    "tb_averages = [statistics.mean(trusting_beliefs_scores[i]) for i in range(len(trusting_beliefs_scores))]\n",
    "tb_median = [statistics.median(trusting_beliefs_scores[i]) for i in range(len(trusting_beliefs_scores))]\n",
    "tb_variance = [statistics.variance(trusting_beliefs_scores[i]) for i in range(len(trusting_beliefs_scores))]\n",
    "\n",
    "\n",
    "et_averages = [statistics.mean(explicit_trust_scores[i]) for i in range(len(explicit_trust_scores))]\n",
    "et_median = [statistics.median(explicit_trust_scores[i]) for i in range(len(explicit_trust_scores))]\n",
    "et_variance = [statistics.variance(explicit_trust_scores[i]) for i in range(len(explicit_trust_scores))]\n",
    "\n",
    "\n",
    "ts_averages = [statistics.mean(md_trust_scores[i]) for i in range(len(md_trust_scores))]\n",
    "ts_median = [statistics.median(md_trust_scores[i]) for i in range(len(md_trust_scores))]\n",
    "ts_variance = [statistics.variance(md_trust_scores[i]) for i in range(len(md_trust_scores))]\n",
    "\n",
    "\n",
    "q16_averages = [statistics.mean(q16_scores[i]) for i in range(len(competence_scores))]\n",
    "q16_median = [statistics.median(q16_scores[i]) for i in range(len(competence_scores))]\n",
    "\n",
    "q17_averages = [statistics.mean(q17_scores[i]) for i in range(len(competence_scores))]\n",
    "q17_median = [statistics.median(q17_scores[i]) for i in range(len(competence_scores))]\n",
    "\n",
    "q18_averages = [statistics.mean(q18_scores[i]) for i in range(len(competence_scores))]\n",
    "q18_median = [statistics.median(q18_scores[i]) for i in range(len(competence_scores))]\n",
    "\n",
    "q19_averages = [statistics.mean(q19_scores[i]) for i in range(len(competence_scores))]\n",
    "q19_median = [statistics.median(q19_scores[i]) for i in range(len(competence_scores))]\n",
    "\n",
    "\n",
    "q16_averages_all_round = statistics.mean(q16_scores[0]+q16_scores[1]+q16_scores[2])\n",
    "q16_median_all_round = statistics.median(q16_scores[0]+q16_scores[1]+q16_scores[2])\n",
    "\n",
    "q17_averages_all_round = statistics.mean(q17_scores[0]+q17_scores[1]+q17_scores[2])\n",
    "q17_median_all_round = statistics.median(q17_scores[0]+q17_scores[1]+q17_scores[2])\n",
    "\n",
    "q18_averages_all_round = statistics.mean(q18_scores[0]+q18_scores[1]+q18_scores[2])\n",
    "q18_median_all_round = statistics.median(q18_scores[0]+q18_scores[1]+q18_scores[2])\n",
    "\n",
    "q19_averages_all_round = statistics.mean(q19_scores[0]+q19_scores[1]+q19_scores[2])\n",
    "q19_median_all_round = statistics.median(q19_scores[0]+q19_scores[1]+q19_scores[2])\n",
    "\n",
    "print(\"Competence\")\n",
    "print(competence_averages)\n",
    "print(competence_median)                                                                        \n",
    "print(competence_variance) \n",
    "print(\"Benevolence\")\n",
    "print(benevolence_averages)\n",
    "print(benevolence_median)                                                                            \n",
    "print(benevolence_variance)                                                                            \n",
    "\n",
    "  \n",
    "print(\"Integrity\")    \n",
    "print(integrity_averages)\n",
    "print(integrity_median)\n",
    "print(integrity_variance)\n",
    "\n",
    "                                                                          \n",
    "                                                                        \n",
    "print(\"Intention to return\")\n",
    "print(itr_averages)\n",
    "print(itr_median)\n",
    "print(itr_variance)\n",
    "print(\"Perceived Transparency\")\n",
    "print(pt_averages)\n",
    "print(pt_median)\n",
    "print(pt_variance)\n",
    "print(\"Trusting Beliefs\")\n",
    "print(tb_averages)\n",
    "print(tb_median)                                                                          \n",
    "print(tb_variance) \n",
    "\n",
    "print(\"Explicit Trust\")\n",
    "print(et_averages)\n",
    "print(et_median)\n",
    "print(et_variance)\n",
    "\n",
    "print(\"MD Trust\")\n",
    "print(ts_averages)\n",
    "print(ts_median)                                                                          \n",
    "print(ts_variance) \n",
    "\n",
    "print(\"Q16\")\n",
    "print(q16_averages)\n",
    "print(q16_median)                                                                          \n",
    "    \n",
    "print(\"Q17\")\n",
    "print(q17_averages)\n",
    "print(q17_median)                                                                          \n",
    "    \n",
    "print(\"Q18\")\n",
    "print(q18_averages)\n",
    "print(q18_median)                                                                          \n",
    "    \n",
    "print(\"Q19\")\n",
    "print(q19_averages)\n",
    "print(q19_median)                                                                          \n",
    "    \n",
    "print(\"Q16 All Round\")\n",
    "print(q16_averages_all_round)\n",
    "print(q16_median_all_round)  \n",
    "\n",
    "print(\"Q17 All Round\")\n",
    "print(q17_averages_all_round)\n",
    "print(q17_median_all_round)  \n",
    "    \n",
    "print(\"Q18 All Round\")\n",
    "print(q18_averages_all_round)\n",
    "print(q18_median_all_round) \n",
    "\n",
    "print(\"Q19 All Round\")\n",
    "print(q19_averages_all_round)\n",
    "print(q19_median_all_round)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
